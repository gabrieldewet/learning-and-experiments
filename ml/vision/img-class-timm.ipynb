{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w93AUoazW3jl"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHtYmk3DW74s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers.image_utils import load_image\n",
        "from transformers import (\n",
        "    AutoImageProcessor,\n",
        "    AutoModelForImageClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DefaultDataCollator,\n",
        ")\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "import evaluate\n",
        "from PIL import Image\n",
        "import json\n",
        "import time\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64N824zzW8fE"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# workdir = \"data/fashion_mnist\"\n",
        "\n",
        "# # Define transformations (convert images to tensors and normalize if needed)\n",
        "# transform = transforms.ToTensor()\n",
        "\n",
        "# # Download the FashionMNIST dataset\n",
        "# fashion_mnist = datasets.FashionMNIST(\n",
        "#     root=\".\",  # Temporary directory to store the raw dataset\n",
        "#     train=True,  # Download the training set\n",
        "#     download=True,  # Download the dataset if not already present\n",
        "#     transform=transform,\n",
        "# )\n",
        "\n",
        "# label_map = {0: \"top\", 1: \"trouser\"}\n",
        "\n",
        "# # Save each image as a separate file\n",
        "# for idx, (image, label) in enumerate(fashion_mnist):\n",
        "#     if label > 1:\n",
        "#         continue\n",
        "#     # Convert the tensor image to a PIL image\n",
        "#     pil_image = transforms.ToPILImage()(image)\n",
        "\n",
        "#     # Create a subdirectory for each label (optional)\n",
        "#     label_dir = os.path.join(workdir, str(label_map[label]))\n",
        "#     os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "#     # Save the image to the corresponding label directory\n",
        "#     image_path = os.path.join(label_dir, f\"image_{idx}.png\")\n",
        "#     pil_image.save(image_path)\n",
        "\n",
        "\n",
        "# print(f\"All images saved to: {workdir}\")\n",
        "\n",
        "# shutil.rmtree(\"FashionMNIST\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create torch dataset, and then huggingfa\n",
        "images_path = Path(\"./data/fashion_mnist/all\")\n",
        "classes = [fold.stem for fold in images_path.glob(\"*\")]\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label2id = {\"top\": 0, \"trouser\": 1}\n",
        "\n",
        "# To transformers dataset\n",
        "all_images = []\n",
        "labels = []\n",
        "\n",
        "with open(\"./data/fashion_mnist/fashion_mnist.jsonl\", \"w\") as f:\n",
        "    id_ = 0\n",
        "    for class_ in classes:\n",
        "        class_path = images_path / class_\n",
        "        for img_path in class_path.glob(\"*.png\"):\n",
        "            new_path = shutil.copy2(img_path.as_posix(), images_path.parent / \"images\")\n",
        "            all_images.append(new_path)\n",
        "            labels.append(label2id[class_])\n",
        "\n",
        "            f.write(json.dumps({\"id\": id_, \"filename\": img_path.name, \"label\": label2id[class_]}) + \"\\n\")\n",
        "            id_ += 1\n",
        "\n",
        "# Create a Hugging Face Dataset\n",
        "dataset = Dataset.from_dict({\"image_path\": all_images, \"label\": labels})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Perform train-test split\n",
        "tv_test_split = dataset.train_test_split(test_size=0.25, seed=SEED)\n",
        "\n",
        "# Add validation split\n",
        "train_val_split = tv_test_split[\"train\"].train_test_split(test_size=tv_test_split[\"test\"].num_rows, seed=SEED)\n",
        "\n",
        "train_set = train_val_split[\"train\"]\n",
        "validation_set = train_val_split[\"test\"]\n",
        "test_set = tv_test_split[\"test\"]\n",
        "\n",
        "splits_dict = {\n",
        "    \"validation\": [Path(img[\"image_path\"]).name for img in validation_set],\n",
        "    \"test\": [Path(img[\"image_path\"]).name for img in test_set],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write split to file\n",
        "with open(\"./data/fashion_mnist/splits.json\", \"w\") as f:\n",
        "    json.dump(splits_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK4JcknYZDBK",
        "outputId": "adc5fa40-85b8-4d39-ebca-60f97892dd3d"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of training examples: {len(train_set)}\")\n",
        "print(f\"Number of validation examples: {len(validation_set)}\")\n",
        "print(f\"Number of test examples: {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6_yv4DfJii5",
        "outputId": "a09e2484-4ec9-429b-cb73-aed90a5861bb"
      },
      "outputs": [],
      "source": [
        "labels = classes\n",
        "num_labels = len(labels)\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "# Build the dictionaries for easier query\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[i] = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmRaiWMGXfiW"
      },
      "source": [
        "### Visualize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i5eYVkuYK-o"
      },
      "outputs": [],
      "source": [
        "def display_image_grid(images, labels, rows=2, cols=5, figsize=(12, 6), target_size=(128, 128), after_aug=False):\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, ax in enumerate(axes):\n",
        "        if after_aug:\n",
        "            image = images[i]\n",
        "            image = image.permute(1, 2, 0)\n",
        "            image = image.clip(min=0.0, max=1.0)\n",
        "        else:\n",
        "            image = images[i].resize((128, 128))\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(labels[i])\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "UTDtibtfYQOA",
        "outputId": "8434daf8-af02-413f-ba55-7213b64841f0"
      },
      "outputs": [],
      "source": [
        "samples = train_set.shuffle().select(range(10))\n",
        "sample_images = [Image.open(img_path) for img_path in samples[\"image_path\"]]\n",
        "sample_labels = [id2label[label] for label in samples[\"label\"]]\n",
        "display_image_grid(sample_images, sample_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN889zW3ck5A"
      },
      "source": [
        "## Dataset Transforms for Traning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pre-trained model from Hugging Face Hub\n",
        "model_path = \"./models/resnet18.a1_in1k\"\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_path)\n",
        "\n",
        "\n",
        "# checkpoint = \"timm/resnet18.a1_in1k\"\n",
        "# model = AutoModelForImageClassification.from_pretrained(checkpoint)\n",
        "# model.save_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E3r5Y1YqsdD"
      },
      "outputs": [],
      "source": [
        "train_transforms = image_processor.train_transforms\n",
        "val_transforms = image_processor.val_transforms\n",
        "\n",
        "\n",
        "def apply_transforms(examples, train_aug=False):\n",
        "    if train_aug:\n",
        "        examples[\"pixel_values\"] = [train_transforms(Image.open(img).convert(\"RGB\")) for img in examples[\"image_path\"]]\n",
        "    else:\n",
        "        examples[\"pixel_values\"] = [val_transforms(Image.open(img).convert(\"RGB\")) for img in examples[\"image_path\"]]\n",
        "\n",
        "    del examples[\"image_path\"]\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLVazBRMqwGu"
      },
      "outputs": [],
      "source": [
        "train_ds = train_set.with_transform(partial(apply_transforms, train_aug=True))\n",
        "test_ds = test_set.with_transform(apply_transforms)\n",
        "val_ds = validation_set.with_transform(apply_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "TNuU9l3frZ-4",
        "outputId": "0cae62dd-6b7e-4330-e687-ab03ecfb2ca3"
      },
      "outputs": [],
      "source": [
        "samples = train_ds.shuffle().select(range(10))\n",
        "sample_images = [s[\"pixel_values\"] for s in samples]\n",
        "sample_labels = [id2label[s[\"label\"]] for s in samples]\n",
        "\n",
        "# After augmentation\n",
        "display_image_grid(sample_images, sample_labels, after_aug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxVHRC77tMj9"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjlf_XOJK7Rf",
        "outputId": "dc41b703-c7df-4825-e659-eda38493d81f"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_path,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l33uBAjpHSLV"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(pred: EvalPrediction):\n",
        "    # Extract predictions and labels\n",
        "    predictions = np.argmax(pred.predictions, axis=1)  # Get the predicted class\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    # Compute metrics (e.g., accuracy, precision, recall, F1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
        "\n",
        "    # Return a dictionary of metrics\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeuTRRRoHT3u"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-5\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "output_dir = \"models/checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTUee6cTPl_Z"
      },
      "outputs": [],
      "source": [
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    remove_unused_columns=False,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=2,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    warmup_ratio=0.2,\n",
        "    logging_steps=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    processing_class=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "i_qGZWvJoT7f",
        "outputId": "214bd494-289a-4da3-cc23-139f5116bcbc"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model and tokenizer\n",
        "output_dir = \"./models/trained-model\"\n",
        "trainer.model.save_pretrained(output_dir)  # Save the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-FFITQ91KjV"
      },
      "source": [
        "## Inference on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp7W1Yus74Mg"
      },
      "outputs": [],
      "source": [
        "image = load_image(\"data/fashion_mnist/images/image_1.png\")\n",
        "inputs = image_processor(image, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk2ajCux72oq"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "    labels = logits.argmax(-1).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "S0U7Vdc__-EE",
        "outputId": "2475ddb0-200f-4cec-e46d-739228dab297"
      },
      "outputs": [],
      "source": [
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Prediction: {id2label[labels]}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test optimised inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_image_classification_inference_time_separate(\n",
        "    image_paths: list[Path],\n",
        "    model_name: str,  # Replace with your model\n",
        "    batch_size=1,\n",
        "    device=torch.accelerator.current_accelerator() if torch.accelerator.is_available() else \"cpu\",\n",
        "):\n",
        "    image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "    model = AutoModelForImageClassification.from_pretrained(model_name).to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_inference_time = 0.0\n",
        "    num_images = len(image_paths)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for i in range(0, num_images, batch_size):\n",
        "            batch_paths = image_paths[i : i + batch_size]\n",
        "            images = [Image.open(path).convert(\"RGB\") for path in batch_paths]\n",
        "\n",
        "            # Preprocess images\n",
        "            inputs = image_processor(images=images, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            start_time = time.time()\n",
        "            _ = model(**inputs)  # Perform inference\n",
        "            end_time = time.time()\n",
        "\n",
        "            total_inference_time += end_time - start_time\n",
        "\n",
        "    average_inference_time = total_inference_time / num_images\n",
        "    return average_inference_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_IMAGES = 1000\n",
        "image_paths = [img for i, img in enumerate(Path(\"data/fashion_mnist/images\").glob(\"*\")) if i < N_IMAGES]\n",
        "\n",
        "print(len(image_paths))\n",
        "\n",
        "time_b1 = measure_image_classification_inference_time_separate(image_paths, output_dir, batch_size=1)\n",
        "time_b2 = measure_image_classification_inference_time_separate(image_paths, output_dir, batch_size=2)\n",
        "\n",
        "print(f\"Time per image with batch=1: {time_b1:.2f} seconds\")\n",
        "print(f\"Time per image with batch=2: {time_b2:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batche inputs to output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image1 = load_image(image_paths[0].as_posix())\n",
        "image2 = load_image(image_paths[1].as_posix())\n",
        "image1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_processor = AutoImageProcessor.from_pretrained(output_dir)\n",
        "# Preprocess both images\n",
        "inputs1 = image_processor(image1, return_tensors=\"pt\")\n",
        "inputs2 = image_processor(image2, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs2.pixel_values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate the pixel values along the batch dimension\n",
        "batched_inputs = {\"pixel_values\": torch.cat((inputs1.pixel_values, inputs2.pixel_values), dim=0)}\n",
        "batched_inputs[\"pixel_values\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModelForImageClassification.from_pretrained(output_dir).to(torch.accelerator.current_accelerator())\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    logits = model(**batched_inputs).logits\n",
        "    predicted_labels = torch.argmax(logits, dim=-1).tolist()  # Get list of predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.argmax(logits, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predicted_labels now contains the predicted label for each image\n",
        "print(f\"Prediction for image 1: {id2label[predicted_labels[0]]}\")\n",
        "print(f\"Prediction for image 2: {id2label[predicted_labels[1]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimization strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark(processor, model, inputs, device):\n",
        "    import time\n",
        "    import psutil\n",
        "\n",
        "    # Warm up\n",
        "    print(\"WARMING UP\")\n",
        "    for img_path in inputs[:2]:\n",
        "        with torch.no_grad():\n",
        "            img = load_image(img_path.as_posix())\n",
        "            input_ = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "            _ = model(**input_)  # Perform inference\n",
        "\n",
        "    process = psutil.Process()\n",
        "\n",
        "    start_time = time.time()\n",
        "    start_memory = process.memory_info().rss\n",
        "    start_cpu_percent = process.cpu_percent(interval=None)  # Non-blocking\n",
        "\n",
        "    track_cpu = []\n",
        "    track_memory = []\n",
        "    # Your code to benchmark (e.g., model inference)\n",
        "    print(\"RUNNING BENCH\")\n",
        "    for img_path in inputs[2:]:\n",
        "        with torch.no_grad():\n",
        "            img = load_image(img_path.as_posix())\n",
        "            input_ = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "            _ = model(**input_)  # Perform inference\n",
        "            track_cpu.append(process.cpu_percent(interval=None))\n",
        "            track_memory.append((process.memory_info().rss - start_memory) / (1024 * 1024))\n",
        "\n",
        "    end_time = time.time()\n",
        "    end_memory = process.memory_info().rss\n",
        "    end_cpu_percent = process.cpu_percent(interval=None)  # Non-blocking\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    memory_usage = end_memory - start_memory\n",
        "    cpu_usage = end_cpu_percent\n",
        "\n",
        "    print(f\"Elapsed time: {elapsed_time:.2f} seconds ({elapsed_time / len(inputs[2:]):.2f} seconds per image)\")\n",
        "    print(f\"CPU Usage: {cpu_usage:.2f}%\")\n",
        "    print(\n",
        "        f\"Memory Usage: {memory_usage / (1024 * 1024):.2f} MB (Min: {min(track_memory):.2f} | Max: {max(track_memory):.2f} | Average: {np.mean(track_memory):.2f})\"\n",
        "    )\n",
        "    del model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.accelerator.current_accelerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reload_model(model_path=output_dir, device=torch.accelerator.current_accelerator()):\n",
        "    image_processor = AutoImageProcessor.from_pretrained(model_path)\n",
        "    model = AutoModelForImageClassification.from_pretrained(model_path).to(device)\n",
        "\n",
        "    return image_processor, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_dtypes(model):\n",
        "    dtypes = []\n",
        "    for _, param in model.named_parameters():\n",
        "        dtypes.append(param.dtype)\n",
        "\n",
        "    return set(dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_processor, model = reload_model()\n",
        "model_dtypes(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Simply half the model float16\n",
        "model = model.half()  # Convert to float16\n",
        "\n",
        "model_dtypes(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark(image_processor, model, image_paths, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtype = torch.float16\n",
        "example_input = image_processor(image1, return_tensors=\"pt\")\n",
        "input_tensor = example_input[\"pixel_values\"].to(device).to(dtype)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    predicted_class = torch.argmax(output.logits, dim=-1)\n",
        "    print(f\"Predicted class: {predicted_class}\")\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_processor, model = reload_model()\n",
        "model.eval()  # Set to evaluation mode\n",
        "model_dtypes(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark(image_processor, model, image_paths, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtype = torch.float16\n",
        "example_input = image_processor(image1, return_tensors=\"pt\")\n",
        "input_tensor = example_input[\"pixel_values\"].to(device)  # .to(dtype)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "    predicted_class = torch.argmax(output.logits, dim=-1)\n",
        "    print(f\"Predicted class: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quantization\n",
        "\n",
        " Convert your model's weights from float32 to int8. This drastically reduces model size and can speed up inference on CPUs that support int8 operations. PyTorch has built-in quantization tools (e.g., `torch.quantization`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()  # Important to set to eval mode\n",
        "\n",
        "# Static Quantization (requires calibration)\n",
        "model.qconfig = torch.quantization.get_default_qconfig()  # Choose appropriate qconfig for your CPU\n",
        "torch.quantization.prepare(model, inplace=True)\n",
        "\n",
        "# Calibration (run some data through the model to collect statistics)\n",
        "# Replace 'calibration_dataset' with a representative subset of your training data\n",
        "with torch.no_grad():\n",
        "    for img_path in image_paths[:10]:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        input_ = image_processor(img)\n",
        "        model(**input_)\n",
        "\n",
        "torch.quantization.convert(model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPaxyMYlIxeNg6kBi6Q1DeR",
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
